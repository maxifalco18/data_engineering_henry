# üöÄ Proyecto Integrador ‚Äì Sistema de An√°lisis de Ventas
---

# üß± Proyecto Integrador ‚Äì Avance 1: Estructura y Carga de Datos

Este repositorio corresponde al **Avance 1** del Proyecto Integrador de Henry. El objetivo de esta fase inicial fue establecer los cimientos de un sistema de an√°lisis de ventas, aplicando conceptos fundamentales de bases de datos, programaci√≥n orientada a objetos (POO) y buenas pr√°cticas de ingenier√≠a de software.

El trabajo realizado en esta etapa es la base sobre la cual se construir√°n las funcionalidades m√°s complejas en fases posteriores.

---

## ‚úÖ Objetivos Cumplidos en esta Fase

-   **Estructurar el proyecto** con una organizaci√≥n de carpetas l√≥gica y escalable.
-   **Modelar las entidades del negocio** (`Product`, `Customer`, etc.) como clases en Python, aplicando principios de POO.
-   **Crear un esquema de base de datos relacional** en PostgreSQL, garantizando la integridad de los datos con claves primarias y for√°neas.
-   **Cargar un conjunto de datos** desde archivos `.csv` a la base de datos de manera eficiente.
-   **Validar la carga de datos** para asegurar que la ingesta fue exitosa.
-   **Implementar pruebas unitarias** simples con `pytest` para verificar la correcta instanciaci√≥n de los modelos.

---

## üóÇÔ∏è Estructura del Proyecto

Se implement√≥ una estructura de directorios est√°ndar en la industria para garantizar la **modularidad y la separaci√≥n de conceptos (Separation of Concerns)**.

```
proyecto_integrador/
‚îú‚îÄ‚îÄ data/                     # Contiene los archivos CSV de entrada.
‚îú‚îÄ‚îÄ src/                      # C√≥digo fuente principal de la aplicaci√≥n.
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py           # Permite que 'src' sea tratado como un paquete de Python.
‚îÇ   ‚îî‚îÄ‚îÄ models.py             # Define las clases POO que representan las tablas.
‚îú‚îÄ‚îÄ sql/                      # Almacena todos los scripts SQL.
‚îÇ   ‚îî‚îÄ‚îÄ load_data.sql         # Script para la creaci√≥n de tablas y carga de datos.
‚îú‚îÄ‚îÄ tests/                    # Contiene las pruebas unitarias del proyecto.
‚îÇ   ‚îî‚îÄ‚îÄ test_models.py        # Pruebas para las clases definidas en models.py.
‚îú‚îÄ‚îÄ .env                      # Archivo para almacenar variables de entorno (no versionado).
‚îú‚îÄ‚îÄ .gitignore                # Especifica los archivos a ignorar por Git.
‚îú‚îÄ‚îÄ requirements.txt          # Lista de dependencias de Python del proyecto.
‚îî‚îÄ‚îÄ README.md                 # Este archivo de documentaci√≥n.
```

---

## üèõÔ∏è Decisiones de Dise√±o y Arquitectura (Fase 1)

#### 1. Elecci√≥n de la Base de Datos: PostgreSQL en Docker
-   **Decisi√≥n:** Se opt√≥ por **PostgreSQL** por ser una base de datos relacional de c√≥digo abierto, robusta y muy potente, ideal para an√°lisis de datos.
-   **Justificaci√≥n:** Utilizar **Docker** para ejecutar la base de datos garantiza un entorno de desarrollo **100% reproducible y aislado**. Cualquier persona que clone el repositorio puede levantar una instancia id√©ntica de la base de datos con un solo comando, eliminando problemas de configuraci√≥n local.

#### 2. Carga de Datos: Comando `COPY`
-   **Decisi√≥n:** Para cargar los datos desde los archivos `.csv` a las tablas de PostgreSQL, se utiliz√≥ el comando `COPY` nativo de PostgreSQL en lugar de m√∫ltiples sentencias `INSERT`.
-   **Justificaci√≥n:** `COPY` es √≥rdenes de magnitud **m√°s r√°pido y eficiente** para cargas masivas, ya que est√° optimizado para leer un flujo de datos directamente desde un archivo. Esto es una pr√°ctica est√°ndar en la ingenier√≠a de datos para la ingesta inicial.

#### 3. Modelado de Datos: Programaci√≥n Orientada a Objetos (POO)
-   **Decisi√≥n:** Cada tabla del esquema relacional (`countries`, `products`, `sales`, etc.) fue modelada como una clase en Python dentro del archivo `src/models.py`.
-   **Justificaci√≥n:** Este enfoque permite trabajar con los datos de una manera m√°s intuitiva y alineada con el negocio. En lugar de manejar filas como tuplas o diccionarios gen√©ricos, operamos con objetos (`Product`, `Customer`) que tienen atributos y m√©todos propios. Esto mejora la legibilidad, el mantenimiento y la reutilizaci√≥n del c√≥digo. Se implementaron m√©todos `__repr__` para facilitar la depuraci√≥n y m√©todos de negocio como `full_name`.

#### 4. Pruebas Unitarias con `pytest`
-   **Decisi√≥n:** Se implementaron pruebas b√°sicas para validar la correcta creaci√≥n de los objetos de nuestras clases.
-   **Justificaci√≥n:** Introducir pruebas desde la primera fase establece una cultura de calidad y robustez. Asegura que los componentes b√°sicos del sistema funcionan como se espera y proporciona una red de seguridad para futuros cambios.

---

## ‚ñ∂Ô∏è C√≥mo Ejecutar y Validar el Avance 1

### 1. Prerrequisitos
-   Tener `git`, `Python 3.8+` y `Docker` instalados.

### 2. Configuraci√≥n del Entorno
```bash
# 1. Clona el repositorio
git clone <URL_DEL_REPO>
cd proyecto_integrador

# 2. Crea y activa un entorno virtual
python -m venv venv
source venv/bin/activate  # En Windows: venv\Scripts\activate

# 3. Instala las dependencias
pip install -r requirements.txt
```

### 3. Levantar la Base de Datos y Cargar los Datos
```bash
# 1. Levanta el contenedor de PostgreSQL con Docker
docker run --name pg-integrador -e POSTGRES_USER=postgres -e POSTGRES_PASSWORD=admin123 -e POSTGRES_DB=proyecto_integrador -p 5432:5432 -d postgres:15

# 2. Copia los archivos CSV al contenedor
docker cp ./data/. pg-integrador:/data

# 3. Ejecuta el script SQL
# Abre tu cliente de base de datos preferido (DBeaver, DataGrip, etc.),
# con√©ctate a la base de datos en Docker y ejecuta el contenido completo
# del archivo 'sql/load_data.sql'.
```

### 4. Correr las Pruebas Unitarias
Para verificar que los modelos POO funcionan correctamente:
```bash
pytest -v tests/test_models.py
```
**Resultado Esperado:** Todas las pruebas deben pasar (`PASSED`), confirmando que los objetos se instancian con los atributos correctos.

---

## üìå Desaf√≠os y Notas Finales

-   **Calidad de Datos:** Durante la carga, se identificaron problemas de formato en las columnas de fecha (`SalesDate`, `ModifyDate`), que conten√≠an valores inv√°lidos.
-   **Soluci√≥n Temporal:** Para permitir la carga inicial, estas columnas se definieron como de tipo `TEXT` en la base de datos. Se document√≥ que este es un punto de "deuda t√©cnica" a resolver en fases posteriores mediante un proceso de limpieza de datos (ETL).
-   **Integridad Referencial:** Se establecieron relaciones con claves for√°neas (`FOREIGN KEY`) en el script de creaci√≥n de tablas para garantizar la consistencia y la integridad de los datos a nivel de base de datos.
-   **Validaci√≥n de Carga:** El script `load_data.sql` incluye sentencias `SELECT COUNT(*)` para cada tabla, permitiendo una validaci√≥n r√°pida y directa de que todos los registros de los CSVs fueron cargados correctamente.

¬°Por supuesto! Aqu√≠ tienes un `README.md` s√∫per completo y descriptivo, enfocado exclusivamente en el **Avance 2**.

Este documento est√° dise√±ado para que un evaluador pueda entender en profundidad las decisiones de arquitectura y dise√±o que tomaste en esta fase, demostrando tu capacidad para construir software modular, mantenible y profesional.

---

# üöÄ Proyecto Integrador ‚Äì Avance 2: Modularizaci√≥n y Patrones de Dise√±o

Este documento detalla los avances correspondientes a la **Fase 2** del Proyecto Integrador. El foco de esta etapa fue transformar la base est√°tica del proyecto en una **aplicaci√≥n Python din√°mica, modular y robusta**. Se aplicaron patrones de dise√±o de software para construir una soluci√≥n escalable y desacoplada, siguiendo las mejores pr√°cticas de la industria.

---

## ‚úÖ Objetivos Cumplidos en esta Fase

-   **Modularizar el sistema** para desacoplar la l√≥gica de negocio del acceso a los datos.
-   Implementar una **clase de conexi√≥n** a la base de datos aplicando el patr√≥n de dise√±o **Singleton** para una gesti√≥n eficiente de recursos.
-   Crear una **capa de acceso a datos** con el patr√≥n **Repository** para centralizar todas las consultas SQL.
-   Ejecutar consultas desde Python y **formatear los resultados como DataFrames de Pandas** para facilitar el an√°lisis.
-   **Proteger las credenciales** de la base de datos de forma segura utilizando un archivo `.env`.
-   **Integrar la arquitectura completa** en un Jupyter Notebook funcional que demuestre el flujo de datos.
-   A√±adir **pruebas unitarias avanzadas** para los patrones implementados, utilizando **mocks** para garantizar pruebas aisladas y r√°pidas.

---

## üóÇÔ∏è Arquitectura de la Aplicaci√≥n (Fase 2)

La estructura del proyecto fue extendida para soportar la nueva l√≥gica de la aplicaci√≥n. Los archivos clave de esta fase son:

```
proyecto_integrador/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 2_analisis_de_ventas.ipynb   # <-- Notebook de integraci√≥n y visualizaci√≥n.
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ database.py                # <-- Gestor de conexi√≥n (Implementa Singleton).
‚îÇ   ‚îú‚îÄ‚îÄ models.py                  # (Sin cambios estructurales desde la Fase 1).
‚îÇ   ‚îî‚îÄ‚îÄ repository.py              # <-- Capa de acceso a datos (Implementa Repository).
‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îú‚îÄ‚îÄ test_database.py           # <-- Prueba unitaria para el patr√≥n Singleton.
‚îÇ   ‚îî‚îÄ‚îÄ test_repository.py         # <-- Prueba unitaria para el Repository (con Mocks).
‚îú‚îÄ‚îÄ .env                           # <-- Archivo para credenciales (local, no en Git).
‚îî‚îÄ‚îÄ requirements.txt               # (Actualizado con nuevas dependencias).
```

---

## üèõÔ∏è Decisiones de Ingenier√≠a y Dise√±o (Fase 2)

En esta fase, se tomaron decisiones de arquitectura clave para elevar la calidad del software.

#### 1. Patr√≥n Singleton para la Conexi√≥n (`src/database.py`)
-   **Problema a Resolver:** Crear una conexi√≥n a la base de datos es una operaci√≥n costosa en t√©rminos de tiempo y recursos. Si cada componente de la aplicaci√≥n (o cada llamada) creara su propia conexi√≥n, el rendimiento se degradar√≠a r√°pidamente y se podr√≠an agotar los recursos del servidor.
-   **Soluci√≥n Implementada:** Se implement√≥ el patr√≥n **Singleton** en la clase `DatabaseConnection`. Este patr√≥n garantiza que, sin importar cu√°ntas veces se solicite una conexi√≥n en la aplicaci√≥n, solo se cree **una √∫nica instancia** del motor de SQLAlchemy. Este motor gestiona un *pool* de conexiones que se reutilizan de manera eficiente.
-   **Justificaci√≥n T√©cnica:** Esta decisi√≥n optimiza el rendimiento, reduce la latencia y asegura una gesti√≥n de recursos predecible y centralizada.

#### 2. Patr√≥n Repository para el Acceso a Datos (`src/repository.py`)
-   **Problema a Resolver:** Mezclar consultas SQL directamente en la l√≥gica de negocio (por ejemplo, dentro de un notebook) crea un c√≥digo fuertemente acoplado. Esto lo hace dif√≠cil de leer, mantener y, sobre todo, probar.
-   **Soluci√≥n Implementada:** Se cre√≥ la clase `DataRepository`, que act√∫a como una **capa de abstracci√≥n** entre la aplicaci√≥n y la base de datos. Esta clase es la √∫nica responsable de construir y ejecutar consultas SQL. Expone m√©todos con nombres claros y relacionados con el negocio (ej. `get_sales_summary_by_country()`).
-   **Justificaci√≥n T√©cnica:** Este patr√≥n **desacopla** la l√≥gica de la aplicaci√≥n de la tecnolog√≠a de la base de datos. Si en el futuro la base de datos cambiara (ej. de PostgreSQL a MySQL) o los datos vinieran de una API, solo tendr√≠amos que modificar el `DataRepository`. El resto de la aplicaci√≥n seguir√≠a funcionando sin cambios, lo que demuestra una arquitectura flexible y mantenible.

#### 3. Pruebas Unitarias con Mocks (`tests/test_repository.py`)
-   **Problema a Resolver:** Las pruebas unitarias deben ser r√°pidas, fiables y no depender de sistemas externos como una base de datos. Probar el `DataRepository` contra una base de datos real ser√≠a una prueba de integraci√≥n, no unitaria, y ser√≠a lenta y fr√°gil.
-   **Soluci√≥n Implementada:** Se utiliz√≥ la librer√≠a `unittest.mock` de Python para **simular (mockear)** la conexi√≥n a la base de datos. Creamos un objeto falso que imita el comportamiento de la conexi√≥n y le indicamos qu√© datos falsos debe devolver.
-   **Justificaci√≥n T√©cnica:** Esto nos permite probar la l√≥gica del `DataRepository` (si construye bien las consultas, si procesa los resultados, etc.) en **total aislamiento**, sin necesidad de tener Docker corriendo o una conexi√≥n de red activa. Las pruebas son r√°pidas, deterministas y se centran en validar una √∫nica unidad de c√≥digo.

---

## ‚ñ∂Ô∏è C√≥mo Ejecutar y Validar el Avance 2

Esta gu√≠a asume que la configuraci√≥n de la **Fase 1** (base de datos en Docker creada y cargada) ha sido completada.

1.  **Actualizar Dependencias:** `pip install -r requirements.txt`
2.  **Configurar `.env`:** Asegurarse de que el archivo `.env` exista en la ra√≠z del proyecto con las credenciales correctas.

#### Ejecutar el An√°lisis de Integraci√≥n:
-   **Acci√≥n:** Abrir y ejecutar todas las celdas del notebook `notebooks/2_analisis_de_ventas.ipynb`.
-   **Resultado Esperado:** El notebook debe ejecutarse de principio a fin sin errores, mostrando mensajes de inicializaci√≥n, un **DataFrame de Pandas** con el resumen de ventas y una **visualizaci√≥n gr√°fica** de los resultados. Esto demuestra que todas las capas de la arquitectura se comunican correctamente.

#### Ejecutar las Pruebas Unitarias:
-   **Acci√≥n:** Correr el siguiente comando en la terminal: `pytest -v tests/`
-   **Resultado Esperado:** Todas las pruebas deben pasar (`PASSED`), confirmando que los patrones Singleton y Repository, as√≠ como su l√≥gica interna, funcionan como se esperaba de forma aislada.

---

## ‚úîÔ∏è Checklist de Requisitos (Fase 2)

| Requisito                            | Estado    | Verificaci√≥n                                     |
| ------------------------------------ | --------- | ------------------------------------------------ |
| Modularizaci√≥n y Patrones de Dise√±o  | ‚úÖ Cumplido | Revisi√≥n de `src/database.py` y `src/repository.py`. |
| Clase de conexi√≥n con Singleton      | ‚úÖ Cumplido | `pytest tests/test_database.py` pasa.            |
| Ejecuci√≥n de consultas desde Python  | ‚úÖ Cumplido | El notebook `2_analisis...` muestra resultados. |
| Resultados como DataFrame de Pandas  | ‚úÖ Cumplido | El notebook muestra un DataFrame.                |
| Notebook de integraci√≥n funcional    | ‚úÖ Cumplido | Ejecuci√≥n completa del notebook sin errores.     |
| Pruebas unitarias para patrones      | ‚úÖ Cumplido | `pytest tests/test_repository.py` pasa.          |
| Seguridad de credenciales con `.env` | ‚úÖ Cumplido | La conexi√≥n funciona leyendo desde `.env`.       |

¬°Absolutamente! Aqu√≠ tienes un `README.md` s√∫per completo y descriptivo, enfocado exclusivamente en el **Avance 3**.

Este documento est√° dise√±ado para que un evaluador pueda entender en profundidad las decisiones de ingenier√≠a, los desaf√≠os y los logros de esta etapa final de optimizaci√≥n, demostrando tu dominio de t√©cnicas avanzadas de SQL y arquitectura de datos.

---

# üöÄ Proyecto Integrador ‚Äì Avance 3: SQL Avanzado y Optimizaci√≥n

Este documento detalla los avances correspondientes a la **Fase 3 y final** del Proyecto Integrador. El objetivo de esta etapa fue refinar y optimizar el sistema, moviendo la l√≥gica de an√°lisis compleja desde la aplicaci√≥n Python hacia la base de datos PostgreSQL.

El principio rector de esta fase fue **"mover el c√≥mputo a los datos, no los datos al c√≥mputo"**. Se implementaron t√©cnicas avanzadas de SQL para mejorar significativamente el rendimiento, la mantenibilidad y la eficiencia del sistema de an√°lisis.

---

## ‚úÖ Objetivos Cumplidos en esta Fase

-   **Crear y ejecutar consultas SQL anal√≠ticas avanzadas** utilizando **CTEs (Common Table Expressions)** y **Funciones de Ventana** (`ROW_NUMBER()`, `LAG()`).
-   **Dise√±ar y crear dos objetos programables en SQL** para encapsular l√≥gica y simplificar el acceso a los datos. Se implementaron una **Vista** y un **Procedimiento Almacenado**.
-   **Integrar la ejecuci√≥n** de estas nuevas consultas y objetos desde la capa de datos de Python (`DataRepository`).
-   **Documentar y presentar los resultados** en el Jupyter Notebook final, explicando las t√©cnicas utilizadas y los insights obtenidos.
-   **Resolver desaf√≠os de calidad de datos** identificados en fases anteriores, espec√≠ficamente con las columnas de fecha.
-   **Actualizar la documentaci√≥n final** del proyecto (`README.md`) para reflejar una arquitectura completa y profesional.

---

## üóÇÔ∏è Arquitectura y Ficheros Clave (Fase 3)

La arquitectura de tres capas se mantuvo, pero la **capa de datos (PostgreSQL)** fue significativamente enriquecida.

```
proyecto_integrador/
‚îú‚îÄ‚îÄ notebooks/
‚îÇ   ‚îî‚îÄ‚îÄ 2_analisis_de_ventas.ipynb   # <-- Actualizado con los an√°lisis de la Fase 3.
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îî‚îÄ‚îÄ repository.py              # <-- Actualizado con nuevos m√©todos para llamar a las queries avanzadas.
‚îú‚îÄ‚îÄ sql/
‚îÇ   ‚îî‚îÄ‚îÄ 3_advanced_objects.sql     # <-- NUEVO: Script para crear la Vista y el Procedimiento Almacenado.
‚îî‚îÄ‚îÄ README.md                      # (Este documento).
```

---

## üèõÔ∏è Decisiones de Ingenier√≠a y Dise√±o (Fase 3)

En esta fase, se deleg√≥ la responsabilidad del an√°lisis complejo a la base de datos, que est√° dise√±ada para manejar estas operaciones de manera √≥ptima.

#### 1. Consultas Anal√≠ticas Avanzadas

Para responder preguntas de negocio que van m√°s all√° de simples agregaciones, se implementaron dos consultas anal√≠ticas clave:

-   **Ranking de Productos por Categor√≠a:**
    -   **Problema:** Identificar los productos "estrella" dentro de cada categor√≠a. Un simple `GROUP BY` no puede rankear resultados dentro de un grupo.
    -   **Soluci√≥n:** Se utiliz√≥ una **CTE** para pre-calcular las ventas por producto y luego la funci√≥n de ventana **`ROW_NUMBER() OVER (PARTITION BY ...)`** para asignar un ranking a cada producto *dentro* de su categor√≠a. Esto permite un filtrado trivial de los "Top N" productos.
    -   **Justificaci√≥n:** Esta t√©cnica es est√°ndar para problemas de ranking y es extremadamente eficiente, ya que evita m√∫ltiples consultas o un procesamiento complejo en Python.

-   **An√°lisis de Crecimiento Mensual:**
    -   **Problema:** Calcular la tasa de crecimiento de ingresos mes a mes, lo que requiere comparar la fila de un mes con la del mes anterior.
    -   **Soluci√≥n:** Se us√≥ la funci√≥n de ventana **`LAG()`** para acceder a los ingresos del mes anterior en la misma fila del mes actual, permitiendo el c√°lculo del crecimiento porcentual directamente en la sentencia `SELECT`.
    -   **Justificaci√≥n:** `LAG()` es la herramienta nativa de SQL para este tipo de an√°lisis de series temporales. Es mucho m√°s performante que traer toda la serie de datos a Pandas para luego hacer un `shift()` y calcular la diferencia.

#### 2. Vista para Simplificaci√≥n de Datos (`v_ventas_detalladas`)

-   **Problema:** Las consultas de an√°lisis a menudo requer√≠an unir 5 o 6 tablas (`sales`, `customers`, `products`, etc.), lo que resultaba en c√≥digo SQL repetitivo, propenso a errores y dif√≠cil de mantener.
-   **Soluci√≥n:** Se cre√≥ una **VISTA** llamada `v_ventas_detalladas`. Una vista es una tabla virtual almacenada como una consulta `SELECT` que contiene todos los `JOINs` pre-configurados.
-   **Justificaci√≥n:** La vista **abstrae la complejidad**. Ahora, en lugar de escribir `JOINs` complejos, cualquier consulta puede simplemente hacer un `SELECT` a esta vista como si fuera una tabla normal. Esto reduce errores, asegura consistencia y simplifica radicalmente el desarrollo de nuevas consultas.

#### 3. Procedimiento Almacenado para L√≥gica de Negocio (`sp_reporte_cliente`)

-   **Problema:** Ciertas operaciones, como generar un reporte completo para un cliente espec√≠fico, representan una l√≥gica de negocio com√∫n que es reutilizable y requiere un par√°metro de entrada.
-   **Soluci√≥n:** Se cre√≥ un **PROCEDIMIENTO ALMACENADO** (implementado como una `FUNCTION` en PostgreSQL) que acepta un `CustomerID` como par√°metro. Toda la l√≥gica para calcular el total gastado, la categor√≠a favorita y otros KPIs del cliente est√° encapsulada dentro de este procedimiento en el servidor.
-   **Justificaci√≥n:**
    -   **Rendimiento:** Reduce el tr√°fico de red, ya que la aplicaci√≥n solo env√≠a un ID y recibe un peque√±o resultado final.
    -   **Mantenibilidad:** La l√≥gica de negocio del "reporte de cliente" est√° en un solo lugar. Si necesita cambiar, se modifica el procedimiento sin tocar la aplicaci√≥n Python.
    -   **Seguridad y Reutilizaci√≥n:** Expone una funcionalidad de negocio clara y segura que puede ser utilizada por diferentes partes de la aplicaci√≥n o incluso por otros sistemas.

---

## üìå Desaf√≠o Clave: Calidad de Datos de Origen

-   **El Problema:** Se confirm√≥ que las columnas de fecha (`SalesDate`, `ModifyDate`) conten√≠an datos de texto con formatos inv√°lidos (ej. `"31:24.2"`), lo que imped√≠a su tratamiento como un tipo `TIMESTAMP` nativo.
-   **La Soluci√≥n Implementada:** Se abord√≥ este problema de "deuda t√©cnica" directamente en la capa de SQL.
    1.  **En la Vista:** Se implement√≥ una expresi√≥n `CASE WHEN ...` que utiliza expresiones regulares para validar el formato de la fecha en cada fila. Si el formato es v√°lido, lo convierte a `TIMESTAMP`; si es inv√°lido, lo convierte a `NULL`, evitando que las consultas fallen.
    2.  **En el Script de Actualizaci√≥n (Experimento):** Se dise√±√≥ un script (`4_update_products_from_cleaned_csv.sql`) que utiliza un patr√≥n de "staging table" para actualizar los datos desde un CSV limpio. Este script contiene la l√≥gica SQL para parsear y reconstruir las fechas a partir de los deltas de tiempo (ej. `HH:MM.f`), demostrando una soluci√≥n completa al problema.

---

## ‚ñ∂Ô∏è C√≥mo Verificar el Avance 3

1.  **Ejecutar los Objetos SQL:** En un cliente SQL, ejecutar el script `sql/3_advanced_objects.sql` para crear la Vista y el Procedimiento Almacenado.
2.  **Ejecutar el Notebook de An√°lisis:** Abrir y ejecutar todas las celdas de `notebooks/2_analisis_de_ventas.ipynb`. Las nuevas secciones al final del notebook demostrar√°n:
    -   La ejecuci√≥n de las consultas de ranking y crecimiento mensual.
    -   La llamada al procedimiento almacenado para generar un reporte de cliente din√°mico.
    -   El uso impl√≠cito de la vista en las nuevas consultas, que ahora se ejecutan sin errores de fecha.

---

## ‚úîÔ∏è Checklist de Requisitos (Fase 3)

| Requisito                               | Estado    | Verificaci√≥n                                     |
| --------------------------------------- | --------- | ------------------------------------------------ |
| Consultas con CTEs y Funciones Ventana  | ‚úÖ Cumplido | M√©todos en `repository.py` y resultados en notebook. |
| Creaci√≥n de Vista                       | ‚úÖ Cumplido | `sql/3_advanced_objects.sql` y su uso en consultas. |
| Creaci√≥n de Procedimiento Almacenado    | ‚úÖ Cumplido | `sql/3_advanced_objects.sql` y su uso desde Python. |
| Integraci√≥n de todo desde Python        | ‚úÖ Cumplido | El notebook se ejecuta y llama a todos los m√©todos. |
| Documentaci√≥n en Notebook               | ‚úÖ Cumplido | Celdas de Markdown explican cada paso del an√°lisis. |
| Documentaci√≥n Final en `README.md`      | ‚úÖ Cumplido | Este mismo documento.                            |